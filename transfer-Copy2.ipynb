{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import BertTokenizer, cached_path, BertForSequenceClassification\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from torch.utils.data import TensorDataset, random_split,RandomSampler, SequentialSampler\n",
    "from collections import namedtuple\n",
    "from torch.utils.data import DataLoader\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import RunningAverage, Accuracy\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.contrib.handlers import CosineAnnealingScheduler, PiecewiseLinear, create_lr_scheduler_with_warmup, ProgressBar\n",
    "import pandas as pd\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv('./NLP/camp_dataset/sim_question_train.txt',sep=\"@@@@@\",encoding='utf-8',header=None)\n",
    "test=pd.read_csv('./NLP/camp_dataset/sim_question_test.txt',sep=\"@@@@@\",encoding='utf-8',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(data):\n",
    "    for i in range(2):\n",
    "        #data[i]=pd.Series([unicodedata.normalize(\"NFKD\", data[i][j]) for j in range(data.shape[1])])\n",
    "        data[i]=data[i].str.replace(\"\\xa0\",\"\")\n",
    "        data[i]=data[i].str.replace(\"$\",\"\")\n",
    "        data[i]=data[i].str.replace(\" {2,5}\",\"\")\n",
    "        data[i]=data[i].str.replace(\"times\",\"*\")\n",
    "        data[i]=data[i].str.replace(\"div\",\"/\")\n",
    "        data[i]=data[i].str.replace(\"frac\",\"/\")\n",
    "        data[i]=data[i].str.replace(\"^\",\" 次方\")\n",
    "        data[i]=data[i].str.replace(\"+\",\" 加\")\n",
    "        data[i]=data[i].str.replace(\"-\",\" 减\")\n",
    "        data[i]=data[i].str.replace(\"\\}\\{\",\"/\")\n",
    "        data[i]=data[i].str.replace(\"[A-Za-z]{2,10}\",\"\") \n",
    "        data[i]=data[i].str.replace(\"\\}\",\"\")\n",
    "        data[i]=data[i].str.replace(\"\\{\",\"\")\n",
    "        #data[i]=data[i].str.replace(\" \",\"\")\n",
    "        data[i]=data[i].str.replace(\"…\",\" 等\")\n",
    "        #data[i]=data[i].str.replace(\"—\",\"\")\n",
    "        #data[i]=data[i].str.replace(\"“|”\",\"\\\"\")\n",
    "        data[i]=data[i].str.replace(\"∵\",\" 因\")\n",
    "        data[i]=data[i].str.replace(\"∴\",\" 故\")\n",
    "        data[i]=data[i].str.replace(\"馒\",\"\")\n",
    "        data[i]=data[i].str.replace(\"茼\",\"\")\n",
    "        data[i]=data[i].str.replace(\"荤\",\"\")\n",
    "        data[i]=data[i].str.slice(0,511)\n",
    "    return data\n",
    "train=norm(train)\n",
    "test=norm(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased',do_lower_case=False,do_basic_tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildlist(data):\n",
    "    L=[]\n",
    "    for i in range(data.shape[0]):\n",
    "        L=L+[[tokenizer.convert_tokens_to_ids(tokenizer.tokenize(str(data.iloc[i,j]))) for j in range(2)]]\n",
    "    return L\n",
    "TRAIN=buildlist(train)\n",
    "TEST=buildlist(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = namedtuple('Config',\n",
    "  field_names=\"embed_dim, hidden_dim, num_max_positions, num_embeddings      , num_heads, num_layers,\" \n",
    "              \"dropout, initializer_range, batch_size, lr, max_norm, n_epochs, n_warmup,\"\n",
    "              \"mlm, gradient_accumulation_steps, device, log_dir, dataset_cache\")\n",
    "args = Config( 320     , 900    , 320             , len(tokenizer.vocab), 8       , 12   ,\n",
    "               0.1    , 0.08             , 32        , 5e-4, 0.8, 50    , 1000    ,\n",
    "               False, 4, \"cuda\" if torch.cuda.is_available() else \"cpu\", \"./\"   , \"./dataset_cache.bin\")\n",
    "AdaptationConfig = namedtuple('AdaptationConfig',\n",
    "  field_names=\"num_classes, dropout, initializer_range, batch_size, lr, max_norm, n_epochs,\"\n",
    "              \"n_warmup, valid_set_prop, gradient_accumulation_steps, device,\"\n",
    "              \"log_dir, dataset_cache, decreasing_factor\")\n",
    "adapt_args = AdaptationConfig(\n",
    "               2          , 0.1    , 0.08             , 32        , 6.5e-4, 0.5   , 50,\n",
    "               10      , 0.1           , 1, \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "               \"./\"   , \"./dataset_cache.bin\", 2.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets={'train':TRAIN,'valid':TEST}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_embeddings, num_max_positions, num_heads, num_layers, dropout, causal):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.tokens_embeddings = nn.Embedding(num_embeddings, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(num_max_positions, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()\n",
    "        self.layer_norms_1, self.layer_norms_2 = nn.ModuleList(), nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.attentions.append(nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout))\n",
    "            self.feed_forwards.append(nn.Sequential(nn.Linear(embed_dim, hidden_dim),\n",
    "                                                    nn.GELU(),\n",
    "                                                    nn.Linear(hidden_dim, embed_dim),\n",
    "                                                     v ))\n",
    "            self.layer_norms_1.append(nn.LayerNorm(embed_dim, eps=1e-8))\n",
    "            self.layer_norms_2.append(nn.LayerNorm(embed_dim, eps=1e-8))\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        \"\"\" x has shape [seq length, batch], padding_mask has shape [batch, seq length] \"\"\"\n",
    "        positions = torch.arange(len(x), device=x.device).unsqueeze(-1)\n",
    "        h = self.tokens_embeddings(x)\n",
    "        h = h + self.position_embeddings(positions).expand_as(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        attn_mask = None\n",
    "        if self.causal:\n",
    "            attn_mask = torch.full((len(x), len(x)), -float('Inf'), device=h.device, dtype=h.dtype)\n",
    "            attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "\n",
    "        for layer_norm_1, attention, layer_norm_2, feed_forward in zip(self.layer_norms_1, self.attentions,\n",
    "                                                                       self.layer_norms_2, self.feed_forwards):\n",
    "            h = layer_norm_1(h)\n",
    "            x, _ = attention(h, h, h, attn_mask=attn_mask, need_weights=False, key_padding_mask=padding_mask)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "\n",
    "            h = layer_norm_2(h)\n",
    "            x = feed_forward(h)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "            #print(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWithClfHead(nn.Module):\n",
    "    def __init__(self, config, fine_tuning_config):\n",
    "        \"\"\" Transformer with a language modeling head on top (tied weights) \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = Transformer(config.embed_dim, config.hidden_dim, config.num_embeddings,\n",
    "                                       config.num_max_positions, config.num_heads, config.num_layers,\n",
    "                                       config.dropout, causal=not config.mlm)\n",
    "\n",
    "        self.classification_head = nn.Linear(config.embed_dim, fine_tuning_config.num_classes)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        \"\"\" initialize weights - nn.MultiheadAttention is already initalized by PyTorch (xavier) \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, nn.LayerNorm)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        if isinstance(module, (nn.Linear, nn.LayerNorm)) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, clf_tokens_mask, clf_labels=None, padding_mask=None):\n",
    "        \"\"\" x has shape [seq length, batch], padding_mask has shape [batch, seq length] \"\"\"\n",
    "        hidden_states = self.transformer(x, padding_mask)\n",
    "        clf_tokens_states = (hidden_states * clf_tokens_mask.unsqueeze(-1).float()).sum(dim=0)\n",
    "        clf_logits = self.classification_head(clf_tokens_states)\n",
    "\n",
    "        if clf_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            loss = loss_fct(clf_logits.view(-1, clf_logits.size(-1)), clf_labels.view(-1))\n",
    "            return clf_logits, loss\n",
    "        return clf_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_name in ['train', 'valid']:   \n",
    "    datasets[split_name] = [[x[0][max(len(x[0])//2-args.num_max_positions*len(x[0])//(len(x[0])+len(x[1]))//2,0):len(x[0])//2+args.num_max_positions*len(x[0])//2//(len(x[0])+len(x[1]))]] + \n",
    "                            [x[1][max(len(x[1])//2-args.num_max_positions*len(x[1])//(len(x[0])+len(x[1]))//2,0):len(x[1])//2+args.num_max_positions*len(x[1])//2//(len(x[0])+len(x[1]))]] \n",
    "                            for x in datasets[split_name]]   \n",
    "    datasets[split_name] = [x[0] + [tokenizer.vocab['[PAD]']] * (args.num_max_positions - len(x[0])-len(x[1]))+ x[1]  \n",
    "                            for x in datasets[split_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets2={'train':0,'test':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets2['train'] = [x[:args.num_max_positions-1]+[tokenizer.vocab['[CLS]']] for x in datasets['train']]\n",
    "#datasets2['test'] = [x[:args.num_max_positions-1]+[tokenizer.vocab['[CLS]']] for x in datasets['valid']]  \n",
    "datasets2['train'] = [[tokenizer.vocab['[CLS]']]+x[:args.num_max_positions-1] for x in datasets['train']]\n",
    "datasets2['test'] = [[tokenizer.vocab['[CLS]']]+x[:args.num_max_positions-1] for x in datasets['valid']] \n",
    "tensor = torch.tensor(datasets2['train'], dtype=torch.long)\n",
    "labels = torch.tensor(train[2], dtype=torch.long) \n",
    "datasets2['train'] = TensorDataset(tensor, labels)\n",
    "datasets2['test'] = TensorDataset(torch.tensor(datasets2['test'], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = int(adapt_args.valid_set_prop * len(datasets['train']))\n",
    "train_size = len(datasets2['train']) - valid_size\n",
    "valid_dataset, train_dataset = random_split(datasets2['train'], [valid_size, train_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=adapt_args.batch_size, shuffle=True,drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=adapt_args.batch_size, shuffle=False,drop_last=True)\n",
    "test_loader = DataLoader(datasets2['test'], batch_size=adapt_args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation_model = TransformerWithClfHead(args,fine_tuning_config=adapt_args).to(adapt_args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ignite\\handlers\\checkpoint.py:369: UserWarning: Argument save_interval is deprecated and should be None. Please, use events filtering instead, e.g. Events.ITERATION_STARTED(every=1000)\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(adaptation_model.parameters(), lr=args.lr)\n",
    "\n",
    "def update(engine, batch):\n",
    "    adaptation_model.train()\n",
    "    batch, labels = (t.to(adapt_args.device) for t in batch)\n",
    "    inputs = batch.transpose(0, 1).contiguous()  # to shape [seq length, batch]\n",
    "    _, loss = adaptation_model(inputs, clf_tokens_mask=(inputs == tokenizer.vocab['[CLS]']), clf_labels=labels,\n",
    "                               padding_mask=(batch == tokenizer.vocab['[PAD]']))\n",
    "    loss = loss / adapt_args.gradient_accumulation_steps\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(adaptation_model.parameters(), adapt_args.max_norm)\n",
    "    if engine.state.iteration % adapt_args.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss.item()\n",
    "trainer = Engine(update)\n",
    "\n",
    "# Evaluation function and evaluator (evaluator output is the input of the metrics)\n",
    "def inference(engine, batch):\n",
    "    adaptation_model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch, labels = (t.to(adapt_args.device) for t in batch)\n",
    "        inputs = batch.transpose(0, 1).contiguous()  # to shape [seq length, batch]\n",
    "        clf_logits = adaptation_model(inputs, clf_tokens_mask=(inputs == tokenizer.vocab['[CLS]']),\n",
    "                                      padding_mask=(batch == tokenizer.vocab['[PAD]']))\n",
    "    return clf_logits, labels\n",
    "evaluator = Engine(inference)\n",
    "\n",
    "# Attache metric to evaluator & evaluation to trainer: evaluate on valid set after each epoch\n",
    "Accuracy().attach(evaluator, \"accuracy\")\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(valid_loader)\n",
    "    print(f\"Validation Epoch: {engine.state.epoch} Error rate: {100*(1 - evaluator.state.metrics['accuracy'])}\")\n",
    "\n",
    "# Learning rate schedule: linearly warm-up to lr and then to zero\n",
    "scheduler = PiecewiseLinear(optimizer, 'lr', [(0, 9e-8), (adapt_args.n_warmup, adapt_args.lr),\n",
    "                                              (len(train_loader)*adapt_args.n_epochs, 9e-8)])\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "\n",
    "# Add progressbar with loss\n",
    "RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")\n",
    "ProgressBar(persist=True).attach(trainer, metric_names=['loss'])\n",
    "\n",
    "# Save checkpoints and finetuning config\n",
    "checkpoint_handler = ModelCheckpoint(adapt_args.log_dir, 'finetuning_checkpoint', save_interval=1, require_empty=False)\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'mymodel': adaptation_model})\n",
    "torch.save(args, os.path.join(adapt_args.log_dir, 'fine_tuning_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "?PiecewiseLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204c5eb279014ecab48d9542228aec04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=506), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 1 Error rate: 50.16741071428572\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Engine run is terminating due to exception: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\lycan\\\\Documents\\\\cu\\\\hackathon\\\\tmps8f_wide' -> './finetuning_checkpoint_mymodel_506.pth'.\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\lycan\\\\Documents\\\\cu\\\\hackathon\\\\tmps8f_wide' -> './finetuning_checkpoint_mymodel_506.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-ac5989058f3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 850\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setup_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Engine run is terminating due to exception: %s.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[1;34m(self, e)\u001b[0m\n\u001b[0;32m    714\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    940\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_terminate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOMPLETED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_fire_event\u001b[1;34m(self, event_name, *event_args, **event_kwargs)\u001b[0m\n\u001b[0;32m    605\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m                 \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\ignite\\handlers\\checkpoint.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, engine, to_save)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"state_dict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_save\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\ignite\\handlers\\checkpoint.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'{}{}_{}{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fname_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_saved\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCheckpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mItem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpriority\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\ignite\\handlers\\checkpoint.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, checkpoint, filename)\u001b[0m\n\u001b[0;32m    279\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m                 \u001b[0mtmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\lycan\\\\Documents\\\\cu\\\\hackathon\\\\tmps8f_wide' -> './finetuning_checkpoint_mymodel_506.pth'"
     ]
    }
   ],
   "source": [
    "trainer.run(train_loader, max_epochs=args.n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
